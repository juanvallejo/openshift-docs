[[admin-guide-diagnostics-tool]]
= Diagnostics Tool
{product-author}
{product-version}
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:

toc::[]

== Overview

The `oc adm diagnostics` command runs a series of checks for error conditions in
the host or cluster. Specifically, it:

* Verifies that the default registry and router are running and correctly
configured.
* Checks `*ClusterRoleBindings*` and `*ClusterRoles*` for consistency with base
policy.
* Checks that all of the client configuration contexts are valid and can be
connected to.
* Checks that SkyDNS is working properly and the pods have SDN connectivity.
* Validates master and node configuration on the host.
* Checks that nodes are running and available.
* Analyzes host logs for known errors.
* Checks that systemd units are configured as expected for the host.


[[admin-guide-using-the-diagnostics-tool]]
== Using the Diagnostics Tool

{product-title} can be deployed in many ways: built from source, included in a
VM image, in a container image, or as enterprise RPMs. Each method implies a
different configuration and environment. To minimize environment assumptions,
the diagnostics were added to the `openshift` binary so that wherever there is
an {product-title} server or client, the diagnostics can run in the exact same
environment.

To use the diagnostics tool, preferably on a master host and as cluster
administrator, run:

----
$ oc adm diagnostics
----

This runs all available diagnostics, skipping any that do not apply. For
example, the *NodeConfigCheck* does not run unless a node configuration is
available. You can also run specific diagnostics by name as you work to address
issues. For example:

----
$ oc adm diagnostics NodeConfigCheck UnitStatus
----

Diagnostics look for configuration files in standard locations:

* Client:
** As indicated by the `$KUBECONFIG` environment variable variable
**  *_~/.kube/config file_*
* Master:
** *_/etc/origin/master/master-config.yaml_*
* Node:
** *_/etc/origin/node/node-config.yaml_*

Non-standard locations can be specified with flags (respectively,
`--config`, `--master-config`, and `--node-config`). If a configuration file
is not found or specified, related diagnostics are skipped.

Consult the output with the `--help` flag for all available options.

[[admin-guide-diagnostics-tool-server-environment]]
== Running Diagnostics in a Server Environment

Master and node diagnostics are most useful in a specific target environment,
which is a deployment of RPMs with Ansible deployment logic. This provides some
diagnostic benefits:

* Master and node configuration is based on a configuration file in a standard
location.
* Systemd units are configured to manage the server(s).
* All components log to journald.

Having configuration files where Ansible places them means that you will
generally not need to specify where to find them. Running `oc adm diagnostics`
without flags will look for master and node configurations in the standard
locations and use them if found; this should make the Ansible-installed use case
as simple as possible. Also, it is easy to specify configuration files that are
not in the expected locations:

----
$ oc adm diagnostics --master-config=<file_path> --node-config=<file_path>
----

Systemd units and logs entries in journald are necessary for the current log
diagnostic logic. For other deployment types, logs may be going into files, to
stdout, or may combine node and master. At this time, for these situations, log
diagnostics are not able to work properly and will be skipped.

[[admin-guide-diagnostics-tool-client-environment]]
== Running Diagnostics in a Client Environment

You may have access as an ordinary user, and/or as a *cluster-admin* user,
and/or may be running on a host where {product-title} master or node servers are
operating. The diagnostics attempt to use as much access as the user has
available.

A client with ordinary access should be able to diagnose its connection
to the master and run a diagnostic pod. If multiple users or masters are
configured, connections will be tested for all, but the diagnostic pod
only runs against the current user, server, or project.

A client with *cluster-admin* access available (for any user, but only the
current master) should be able to diagnose the status of infrastructure such as
nodes, registry, and router. In each case, running `oc adm diagnostics` looks
for the client configuration in its standard location and uses it if available.

[[additional-cluster-health-checks]]
== Additional Diagnostic Checks

Some additional diagnostic checks are available through the https://hub.docker.com/r/openshift/openshift-ansible[openshift-ansible]
Docker image. These checks are a set of diagnostic tasks meant to be run against a running OpenShift cluster.
They are used to ensure Etcd cluster storage sanity, that correct package versions exist on the host machine, and to ensure a working logging stack deployment.


An action plugin for the OpenShift Health Checks role automatically discovers all diagnostic checks, and executes only those selected in a play.
Some checks have tags, which allow for groups of related checks to be run together as part of a play.

The following table describes available health checks that can be run on a deployed OpenShift cluster.

[[diagnostic-checks]]
.Diagnostic Checks
[options="header"]
|===

|Check Name |Purpose |Tags

|`*ovs_version*`
|This check ensures that a host has the correct version of Open vSwitch installed for the currently deployed version of OpenShift.
|`health`

|`*kibana*`, `*curator*`, `*elasticsearch*`, `*fluentd*`
|This set of checks verifies that `Elasticsearch`, `Fluentd`, and `Curator` pods have been deployed and are in a `running` state, and that a connection can be established between the control host and the exposed `Kibana` URL.

A check for each component can be run on its own, or the entire  set of checks for the logging stack can be run by using the `logging` tag.
|`health`, `logging`

|`*etcd_imagedata_size*`
|This check measures the total size of OpenShift image data in an Etcd cluster. Fails if the calculated size exceeds a user-defined limit. If no limit is specified, this check will fail if the size of OpenShift image data amounts to `50%`
or more of the currently used space in the Etcd cluster.

A user-defined limit may be set by passing the variable `etcd_max_image_data_size_bytes=10000` to the `openshift_health_checker` role.
|`etcd`

|`*etcd_volume*`
|This check ensures that the volume usage for an Etcd cluster is below a maximum user-specified threshold. If no maximum threshold value is specified, it is defaulted to `90%` of the total volume size.

A user-defined limit may be set by passing the variable `etcd_device_usage_threshold_percent=90` to the `openshift_health_checker` role.
|`health`, `etcd`

|`*docker_storage*`
|Only runs on checks that depend on Docker (nodes and containerized installations). Checks that total thinpool usage for the logical volume does not exceed a user defined limit.
If no user-defined limit is set, the maximum thinpool usage threshold defaults to `90%` of the total size available.
The threshold limit for total thinpool percent usage can be set with a variable in your inventory file: `max_thinpool_data_usage_percent: 90`
|`health`, `preflight`
|===

A similar set of checks, meant to run as part of the installation process can be found in xref:../../install_config/install/advanced_install.adoc#configuring-cluster-preflight-checks[Configuring Cluster Preflight Checks].